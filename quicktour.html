<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="generator" content="ExDoc v0.30.2">
    <meta name="project" content="Tokenizers v0.4.0-dev">

    <title>Quicktour ‚Äî Tokenizers v0.4.0-dev</title>
    <link rel="stylesheet" href="dist/html-elixir-JUCHUGAD.css" />


    <script src="dist/handlebars.runtime-NWIB6V2M.js"></script>
    <script src="dist/handlebars.templates-NBND3S2D.js"></script>
    <script src="dist/sidebar_items-1493F6F6.js"></script>

      <script src="docs_config.js"></script>

    <script async src="dist/html-WC6MMJK3.js"></script>


  </head>
  <body data-type="extras" class="page-livemd">
    <script>

      try {
        var settings = JSON.parse(localStorage.getItem('ex_doc:settings') || '{}');

        if (settings.theme === 'dark' ||
           ((settings.theme === 'system' || settings.theme == null) &&
             window.matchMedia('(prefers-color-scheme: dark)').matches)
           ) {
          document.body.classList.add('dark')
        }
      } catch (error) { }
    </script>

<div class="main">

<button class="sidebar-button sidebar-toggle" aria-label="toggle sidebar">
  <i class="ri-menu-line ri-lg" title="Collapse/expand sidebar"></i>
</button>

<section class="sidebar">
  <form class="sidebar-search" action="search.html">
    <button type="submit" class="search-button" aria-label="Submit Search">
      <i class="ri-search-2-line" aria-hidden="true" title="Submit search"></i>
    </button>
    <button type="button" tabindex="-1" class="search-close-button" aria-label="Cancel Search">
      <i class="ri-close-line ri-lg" aria-hidden="true" title="Cancel search"></i>
    </button>
    <label class="search-label">
      <p class="sr-only">Search</p>
      <input name="q" type="text" class="search-input" placeholder="Search..." aria-label="Input your search terms" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" />
    </label>
  </form>

  <div class="autocomplete">
    <div class="autocomplete-results">
    </div>
  </div>

  <div class="sidebar-header">

    <div class="sidebar-projectDetails">
      <a href="Tokenizers.html" class="sidebar-projectName" translate="no">
Tokenizers
      </a>
      <div class="sidebar-projectVersion" translate="no">
        v0.4.0-dev
      </div>
    </div>
    <ul class="sidebar-listNav">
      <li><a id="extras-list-link" href="#full-list">Pages</a></li>

        <li><a id="modules-list-link" href="#full-list">Modules</a></li>


    </ul>
  </div>

  <div class="gradient"></div>
  <ul id="full-list"></ul>
</section>

<section class="content">
  <output role="status" id="toast"></output>
  <div class="content-outer">
    <div id="content" class="content-inner">

<h1>
<button class="icon-action display-settings">
  <i class="ri-settings-3-line"></i>
  <span class="sr-only">Settings</span>
</button>


    <a href="https://github.com/elixir-nx/tokenizers/blob/v0.4.0-dev/notebooks/quicktour.livemd#L1" title="View Source" class="icon-action" rel="help">
      <i class="ri-code-s-slash-line" aria-hidden="true"></i>
      <span class="sr-only">View Source</span>
    </a>


  <span>Quicktour</span>
</h1>

  <div class="livebook-badge-container">
    <a href="#" class="livebook-badge">
      <img src="https://livebook.dev/badge/v1/blue.svg" alt="Run in Livebook" width="150" />
    </a>
  </div>

<pre><code class="makeup elixir" translate="no"><span class="nc">Mix</span><span class="o">.</span><span class="n">install</span><span class="p" data-group-id="0333206259-1">(</span><span class="p" data-group-id="0333206259-2">[</span><span class="w">
  </span><span class="p" data-group-id="0333206259-3">{</span><span class="ss">:tokenizers</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;~&gt; 0.13&quot;</span><span class="p" data-group-id="0333206259-3">}</span><span class="p">,</span><span class="w">
  </span><span class="p" data-group-id="0333206259-4">{</span><span class="ss">:req</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;~&gt; 0.3.8&quot;</span><span class="p" data-group-id="0333206259-4">}</span><span class="w">
</span><span class="p" data-group-id="0333206259-2">]</span><span class="p" data-group-id="0333206259-1">)</span></code></pre><h2 id="intro" class="section-heading">
  <a href="#intro">
    <i class="ri-link-m" aria-hidden="true"></i>
    Intro
  </a>
</h2>
<p>Let‚Äôs have a quick look at the ü§ó Tokenizers library features. The library provides an implementation of today‚Äôs most used tokenizers that is both easy to use and blazing fast.</p><!-- livebook:{"branch_parent_index":0} --><h2 id="downloading-the-data" class="section-heading">
  <a href="#downloading-the-data">
    <i class="ri-link-m" aria-hidden="true"></i>
    Downloading the data
  </a>
</h2>
<p>To illustrate how fast the ü§ó Tokenizers library is, let‚Äôs train a new tokenizer on wikitext-103 (516M of text) in just a few seconds. First things first, you will need to download this dataset and unzip it with:</p><pre><code class="makeup bash" translate="no"><span class="">wget https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-raw-v1.zip
</span><span class="">unzip wikitext-103-raw-v1.zip
</span></code></pre><pre><code class="makeup elixir" translate="no"><span class="nc">Req</span><span class="o">.</span><span class="n">get!</span><span class="p" data-group-id="7328544593-1">(</span><span class="s">&quot;https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-raw-v1.zip&quot;</span><span class="p" data-group-id="7328544593-1">)</span><span class="o">.</span><span class="n">body</span><span class="w">
</span><span class="o">|&gt;</span><span class="w"> </span><span class="nc">Enum</span><span class="o">.</span><span class="n">each</span><span class="p" data-group-id="7328544593-2">(</span><span class="k" data-group-id="7328544593-3">fn</span><span class="w"> </span><span class="p" data-group-id="7328544593-4">{</span><span class="n">filename</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="p" data-group-id="7328544593-4">}</span><span class="w"> </span><span class="o">-&gt;</span><span class="w">
  </span><span class="n">filename</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">to_string</span><span class="p" data-group-id="7328544593-5">(</span><span class="n">filename</span><span class="p" data-group-id="7328544593-5">)</span><span class="w">
  </span><span class="n">path</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nc">Path</span><span class="o">.</span><span class="n">join</span><span class="p" data-group-id="7328544593-6">(</span><span class="bp">__DIR__</span><span class="p">,</span><span class="w"> </span><span class="n">filename</span><span class="p" data-group-id="7328544593-6">)</span><span class="w">
  </span><span class="nc">IO</span><span class="o">.</span><span class="n">puts</span><span class="p" data-group-id="7328544593-7">(</span><span class="s">&quot;Writing </span><span class="si" data-group-id="7328544593-8">#{</span><span class="n">filename</span><span class="si" data-group-id="7328544593-8">}</span><span class="s"> to path </span><span class="si" data-group-id="7328544593-9">#{</span><span class="n">path</span><span class="si" data-group-id="7328544593-9">}</span><span class="s">&quot;</span><span class="p" data-group-id="7328544593-7">)</span><span class="w">

  </span><span class="ss">:ok</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nc">File</span><span class="o">.</span><span class="n">mkdir_p!</span><span class="p" data-group-id="7328544593-10">(</span><span class="nc">Path</span><span class="o">.</span><span class="n">dirname</span><span class="p" data-group-id="7328544593-11">(</span><span class="n">path</span><span class="p" data-group-id="7328544593-11">)</span><span class="p" data-group-id="7328544593-10">)</span><span class="w">
  </span><span class="nc">File</span><span class="o">.</span><span class="n">write!</span><span class="p" data-group-id="7328544593-12">(</span><span class="n">path</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="p">,</span><span class="w"> </span><span class="p" data-group-id="7328544593-13">[</span><span class="ss">:write</span><span class="p" data-group-id="7328544593-13">]</span><span class="p" data-group-id="7328544593-12">)</span><span class="w">
</span><span class="k" data-group-id="7328544593-3">end</span><span class="p" data-group-id="7328544593-2">)</span></code></pre><!-- livebook:{"branch_parent_index":0} --><h2 id="build-a-tokenizer-from-scratch" class="section-heading">
  <a href="#build-a-tokenizer-from-scratch">
    <i class="ri-link-m" aria-hidden="true"></i>
    Build a tokenizer from scratch
  </a>
</h2>
<h3 id="training-the-tokenizer" class="section-heading">
  <a href="#training-the-tokenizer">
    <i class="ri-link-m" aria-hidden="true"></i>
    Training the tokenizer
  </a>
</h3>
<pre><code class="makeup elixir" translate="no"><span class="kn">alias</span><span class="w"> </span><span class="nc">Tokenizers.Tokenizer</span><span class="w">
</span><span class="kn">alias</span><span class="w"> </span><span class="nc">Tokenizers.Trainer</span><span class="w">
</span><span class="kn">alias</span><span class="w"> </span><span class="nc">Tokenizers.PostProcessor</span><span class="w">
</span><span class="kn">alias</span><span class="w"> </span><span class="nc">Tokenizers.PreTokenizer</span><span class="w">
</span><span class="kn">alias</span><span class="w"> </span><span class="nc">Tokenizers.Model</span><span class="w">
</span><span class="kn">alias</span><span class="w"> </span><span class="nc">Tokenizers.Encoding</span></code></pre><p>In this tour, we will build and train a Byte-Pair Encoding (BPE) tokenizer. For more information about the different type of tokenizers, check out this guide in the ü§ó Transformers documentation. Here, training the tokenizer means it will learn merge rules by:</p><ul><li>Start with all the characters present in the training corpus as tokens.</li><li>Identify the most common pair of tokens and merge it into one token.</li><li>Repeat until the vocabulary (e.g., the number of tokens) has reached the size we want.</li></ul><p>The main API of the library is the class Tokenizer, here is how we instantiate one with a BPE model:</p><pre><code class="makeup elixir" translate="no"><span class="p" data-group-id="2474257776-1">{</span><span class="ss">:ok</span><span class="p">,</span><span class="w"> </span><span class="n">model</span><span class="p" data-group-id="2474257776-1">}</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nc">Model.BPE</span><span class="o">.</span><span class="n">init</span><span class="p" data-group-id="2474257776-2">(</span><span class="p" data-group-id="2474257776-3">%{</span><span class="p" data-group-id="2474257776-3">}</span><span class="p">,</span><span class="w"> </span><span class="p" data-group-id="2474257776-4">[</span><span class="p" data-group-id="2474257776-4">]</span><span class="p">,</span><span class="w"> </span><span class="ss">unk_token</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;[UNK]&quot;</span><span class="p" data-group-id="2474257776-2">)</span><span class="w">
</span><span class="p" data-group-id="2474257776-5">{</span><span class="ss">:ok</span><span class="p">,</span><span class="w"> </span><span class="n">tokenizer</span><span class="p" data-group-id="2474257776-5">}</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nc">Tokenizer</span><span class="o">.</span><span class="n">init</span><span class="p" data-group-id="2474257776-6">(</span><span class="n">model</span><span class="p" data-group-id="2474257776-6">)</span></code></pre><p>To train our tokenizer on the wikitext files, we will need to instantiate a <strong>trainer</strong>, in this case a <code class="inline">BpeTrainer</code></p><pre><code class="makeup elixir" translate="no"><span class="p" data-group-id="7646536979-1">{</span><span class="ss">:ok</span><span class="p">,</span><span class="w"> </span><span class="n">trainer</span><span class="p" data-group-id="7646536979-1">}</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nc">Trainer</span><span class="o">.</span><span class="n">bpe</span><span class="p" data-group-id="7646536979-2">(</span><span class="ss">special_tokens</span><span class="p">:</span><span class="w"> </span><span class="p" data-group-id="7646536979-3">[</span><span class="s">&quot;[UNK]&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;[CLS]&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;[SEP]&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;[PAD]&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;[MASK]&quot;</span><span class="p" data-group-id="7646536979-3">]</span><span class="p" data-group-id="7646536979-2">)</span></code></pre><p>We can set the training arguments like <code class="inline">vocab_size</code> or <code class="inline">min_frequency</code> (here left at their default values of <code class="inline">30,000</code> and <code class="inline">0</code>) but the most important part is to give the <code class="inline">special_tokens</code> we plan to use later on (they are not used at all during training) so that they get inserted in the vocabulary.</p><blockquote><p>The order in which you write the special tokens list matters: here <code class="inline">&quot;[UNK]&quot;</code> will get the ID <code class="inline">0</code>, <code class="inline">&quot;[CLS]&quot;</code> will get the ID <code class="inline">1</code> and so forth.</p></blockquote><p>We could train our tokenizer right now, but it wouldn't be optimal. Without a pre-tokenizer that will split our inputs into words, we might get tokens that overlap several words: for instance we could get an &quot;it is&quot; token since those two words often appear next to each other. Using a pre-tokenizer will ensure no token is bigger than a word returned by the pre-tokenizer. Here we want to train a subword BPE tokenizer, and we will use the easiest pre-tokenizer possible by splitting on whitespace.</p><pre><code class="makeup elixir" translate="no"><span class="n">tokenizer</span><span class="w">
</span><span class="o">|&gt;</span><span class="w"> </span><span class="nc">Tokenizer</span><span class="o">.</span><span class="n">set_pre_tokenizer</span><span class="p" data-group-id="1867734340-1">(</span><span class="nc">PreTokenizer</span><span class="o">.</span><span class="n">whitespace</span><span class="p" data-group-id="1867734340-2">(</span><span class="p" data-group-id="1867734340-2">)</span><span class="p" data-group-id="1867734340-1">)</span></code></pre><p>Now, we can just call the <code class="inline">Tokenizer.train</code> method with any list of files we want to use:</p><pre><code class="makeup elixir" translate="no"><span class="p" data-group-id="0605813709-1">[</span><span class="w">
  </span><span class="s">&quot;wikitext-103-raw/wiki.test.raw&quot;</span><span class="p">,</span><span class="w">
  </span><span class="s">&quot;wikitext-103-raw/wiki.train.raw&quot;</span><span class="p">,</span><span class="w">
  </span><span class="s">&quot;wikitext-103-raw/wiki.valid.raw&quot;</span><span class="w">
</span><span class="p" data-group-id="0605813709-1">]</span><span class="w">
</span><span class="o">|&gt;</span><span class="w"> </span><span class="nc">Enum</span><span class="o">.</span><span class="n">map</span><span class="p" data-group-id="0605813709-2">(</span><span class="o">&amp;</span><span class="nc">Path</span><span class="o">.</span><span class="n">join</span><span class="p" data-group-id="0605813709-3">(</span><span class="bp">__DIR__</span><span class="p">,</span><span class="w"> </span><span class="ni">&amp;1</span><span class="p" data-group-id="0605813709-3">)</span><span class="p" data-group-id="0605813709-2">)</span><span class="w">
</span><span class="o">|&gt;</span><span class="w"> </span><span class="n">then</span><span class="p" data-group-id="0605813709-4">(</span><span class="o">&amp;</span><span class="nc">Tokenizer</span><span class="o">.</span><span class="n">train_from_files</span><span class="p" data-group-id="0605813709-5">(</span><span class="n">tokenizer</span><span class="p">,</span><span class="w"> </span><span class="ni">&amp;1</span><span class="p">,</span><span class="w"> </span><span class="n">trainer</span><span class="p" data-group-id="0605813709-5">)</span><span class="p" data-group-id="0605813709-4">)</span></code></pre><p>This should only take a few seconds to train our tokenizer on the full wikitext dataset! To save the tokenizer in one file that contains all its configuration and vocabulary, just use the Tokenizer.save method:</p><pre><code class="makeup elixir" translate="no"><span class="nc">Tokenizer</span><span class="o">.</span><span class="n">save</span><span class="p" data-group-id="1905135238-1">(</span><span class="n">tokenizer</span><span class="p">,</span><span class="w"> </span><span class="nc">Path</span><span class="o">.</span><span class="n">join</span><span class="p" data-group-id="1905135238-2">(</span><span class="bp">__DIR__</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;tokenizer-wiki.json&quot;</span><span class="p" data-group-id="1905135238-2">)</span><span class="p" data-group-id="1905135238-1">)</span></code></pre><p>and you can reload your tokenizer from that file with the <code class="inline">Tokenizer.from_file</code> classmethod:</p><pre><code class="makeup elixir" translate="no"><span class="p" data-group-id="7765600128-1">{</span><span class="ss">:ok</span><span class="p">,</span><span class="w"> </span><span class="n">tokenizer</span><span class="p" data-group-id="7765600128-1">}</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nc">Tokenizer</span><span class="o">.</span><span class="n">from_file</span><span class="p" data-group-id="7765600128-2">(</span><span class="nc">Path</span><span class="o">.</span><span class="n">join</span><span class="p" data-group-id="7765600128-3">(</span><span class="bp">__DIR__</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;tokenizer-wiki.json&quot;</span><span class="p" data-group-id="7765600128-3">)</span><span class="p" data-group-id="7765600128-2">)</span></code></pre><h3 id="using-the-tokenizer" class="section-heading">
  <a href="#using-the-tokenizer">
    <i class="ri-link-m" aria-hidden="true"></i>
    Using the tokenizer
  </a>
</h3>
<!-- livebook:{"break_markdown":true} --><p>Now that we have trained a tokenizer, we can use it on any text we want with the <code class="inline">Tokenizer.encode</code> method:</p><pre><code class="makeup elixir" translate="no"><span class="p" data-group-id="9583312140-1">{</span><span class="ss">:ok</span><span class="p">,</span><span class="w"> </span><span class="n">encoding</span><span class="p" data-group-id="9583312140-1">}</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nc">Tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p" data-group-id="9583312140-2">(</span><span class="n">tokenizer</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;Hello, y&#39;all! How are you üòÅ ?&quot;</span><span class="p" data-group-id="9583312140-2">)</span></code></pre><p>This applied the full pipeline of the tokenizer on the text, returning an Encoding object. To learn more about this pipeline, and how to apply (or customize) parts of it, check out this page.</p><p>This Encoding object then has all the attributes you need for your deep learning model (or other). The tokens attribute contains the segmentation of your text in tokens:</p><pre><code class="makeup elixir" translate="no"><span class="nc">Encoding</span><span class="o">.</span><span class="n">get_tokens</span><span class="p" data-group-id="1061646156-1">(</span><span class="n">encoding</span><span class="p" data-group-id="1061646156-1">)</span></code></pre><p>Similarly, the ids attribute will contain the index of each of those tokens in the tokenizer‚Äôs vocabulary:</p><pre><code class="makeup elixir" translate="no"><span class="nc">Encoding</span><span class="o">.</span><span class="n">get_ids</span><span class="p" data-group-id="3860372820-1">(</span><span class="n">encoding</span><span class="p" data-group-id="3860372820-1">)</span></code></pre><p>An important feature of the ü§ó Tokenizers library is that it comes with full alignment tracking, meaning you can always get the part of your original sentence that corresponds to a given token. Those are stored in the offsets attribute of our Encoding object. For instance, let‚Äôs assume we would want to find back what caused the &quot;[UNK]&quot; token to appear, which is the token at index 9 in the list, we can just ask for the offset at the index:</p><pre><code class="makeup elixir" translate="no"><span class="p" data-group-id="8835957134-1">{</span><span class="n">emoji_offset_start</span><span class="p">,</span><span class="w"> </span><span class="n">emoji_offset_end</span><span class="p" data-group-id="8835957134-1">}</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nc">Encoding</span><span class="o">.</span><span class="n">get_offsets</span><span class="p" data-group-id="8835957134-2">(</span><span class="n">encoding</span><span class="p" data-group-id="8835957134-2">)</span><span class="w"> </span><span class="o">|&gt;</span><span class="w"> </span><span class="nc">Enum</span><span class="o">.</span><span class="n">at</span><span class="p" data-group-id="8835957134-3">(</span><span class="mi">9</span><span class="p" data-group-id="8835957134-3">)</span></code></pre><p>and those are the indices that correspond to the emoji in the original sentence:</p><pre><code class="makeup elixir" translate="no"><span class="s">&quot;Hello, y&#39;all! How are you üòÅ ?&quot;</span><span class="w">
</span><span class="o">|&gt;</span><span class="w"> </span><span class="nc">:binary</span><span class="o">.</span><span class="n">part</span><span class="p" data-group-id="4374222542-1">(</span><span class="w">
  </span><span class="n">emoji_offset_start</span><span class="p">,</span><span class="w">
  </span><span class="c1"># Length</span><span class="w">
  </span><span class="n">emoji_offset_end</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">emoji_offset_start</span><span class="w">
</span><span class="p" data-group-id="4374222542-1">)</span></code></pre><h3 id="post-processing" class="section-heading">
  <a href="#post-processing">
    <i class="ri-link-m" aria-hidden="true"></i>
    Post-processing
  </a>
</h3>
<!-- livebook:{"break_markdown":true} --><p>We might want our tokenizer to automatically add special tokens, like <code class="inline">[CLS]</code> or <code class="inline">[SEP]</code>. To do this, we use a post-processor. TemplateProcessing is the most commonly used, you just have to specify a template for the processing of single sentences and pairs of sentences, along with the special tokens and their IDs.</p><p>When we built our tokenizer, we set <code class="inline">[CLS]</code> and <code class="inline">[SEP]</code> in positions 1 and 2 of our list of special tokens, so this should be their IDs. To double-check, we can use the <code class="inline">Tokenizer.token_to_id</code> method:</p><pre><code class="makeup elixir" translate="no"><span class="nc">Tokenizer</span><span class="o">.</span><span class="n">token_to_id</span><span class="p" data-group-id="6045805665-1">(</span><span class="n">tokenizer</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;[SEP]&quot;</span><span class="p" data-group-id="6045805665-1">)</span></code></pre><p>Here is how we can set the post-processing to give us the traditional BERT inputs:</p><pre><code class="makeup elixir" translate="no"><span class="n">tokenizer</span><span class="w">
</span><span class="o">|&gt;</span><span class="w"> </span><span class="nc">Tokenizer</span><span class="o">.</span><span class="n">set_post_processor</span><span class="p" data-group-id="3363045291-1">(</span><span class="w">
  </span><span class="nc">PostProcessor</span><span class="o">.</span><span class="n">template</span><span class="p" data-group-id="3363045291-2">(</span><span class="w">
    </span><span class="ss">single</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;[CLS] $A [SEP]&quot;</span><span class="p">,</span><span class="w">
    </span><span class="ss">pair</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;[CLS] $A [SEP] $B:1 [SEP]:1&quot;</span><span class="p">,</span><span class="w">
    </span><span class="ss">special_tokens</span><span class="p">:</span><span class="w"> </span><span class="p" data-group-id="3363045291-3">[</span><span class="w">
      </span><span class="p" data-group-id="3363045291-4">{</span><span class="s">&quot;[CLS]&quot;</span><span class="p">,</span><span class="w"> </span><span class="nc">Tokenizer</span><span class="o">.</span><span class="n">token_to_id</span><span class="p" data-group-id="3363045291-5">(</span><span class="n">tokenizer</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;[CLS]&quot;</span><span class="p" data-group-id="3363045291-5">)</span><span class="p" data-group-id="3363045291-4">}</span><span class="p">,</span><span class="w">
      </span><span class="p" data-group-id="3363045291-6">{</span><span class="s">&quot;[SEP]&quot;</span><span class="p">,</span><span class="w"> </span><span class="nc">Tokenizer</span><span class="o">.</span><span class="n">token_to_id</span><span class="p" data-group-id="3363045291-7">(</span><span class="n">tokenizer</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;[SEP]&quot;</span><span class="p" data-group-id="3363045291-7">)</span><span class="p" data-group-id="3363045291-6">}</span><span class="w">
    </span><span class="p" data-group-id="3363045291-3">]</span><span class="w">
  </span><span class="p" data-group-id="3363045291-2">)</span><span class="w">
</span><span class="p" data-group-id="3363045291-1">)</span></code></pre><p>Let's go over this snippet of code in more details. First we specify the template for single sentences: those should have the form <code class="inline">&quot;[CLS] $A [SEP]&quot;</code> where <code class="inline">$A</code> represents our sentence.</p><p>Then, we specify the template for sentence pairs, which should have the form <code class="inline">&quot;[CLS] $A [SEP] $B [SEP]&quot;</code> where <code class="inline">$A</code> represents the first sentence and <code class="inline">$B</code> the second one. The <code class="inline">:1</code> added in the template represent the type IDs we want for each part of our input: it defaults to <code class="inline">0</code> for everything (which is why we don't have <code class="inline">$A:0</code>) and here we set it to 1 for the tokens of the second sentence and the last <code class="inline">&quot;[SEP]&quot;</code> token.</p><p>Lastly, we specify the special tokens we used and their IDs in our tokenizer's vocabulary.</p><p>To check out this worked properly, let's try to encode the same sentence as before:</p><pre><code class="makeup elixir" translate="no"><span class="p" data-group-id="4741442391-1">{</span><span class="ss">:ok</span><span class="p">,</span><span class="w"> </span><span class="n">encoding</span><span class="p" data-group-id="4741442391-1">}</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nc">Tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p" data-group-id="4741442391-2">(</span><span class="n">tokenizer</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;Hello, y&#39;all! How are you üòÅ ?&quot;</span><span class="p" data-group-id="4741442391-2">)</span><span class="w">
</span><span class="nc">Encoding</span><span class="o">.</span><span class="n">get_tokens</span><span class="p" data-group-id="4741442391-3">(</span><span class="n">encoding</span><span class="p" data-group-id="4741442391-3">)</span></code></pre><p>To check the results on a pair of sentences, we just pass the two sentences to <code class="inline">Tokenizer.encode</code>:</p><pre><code class="makeup elixir" translate="no"><span class="p" data-group-id="0429442747-1">{</span><span class="ss">:ok</span><span class="p">,</span><span class="w"> </span><span class="n">encoding</span><span class="p" data-group-id="0429442747-1">}</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nc">Tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p" data-group-id="0429442747-2">(</span><span class="n">tokenizer</span><span class="p">,</span><span class="w"> </span><span class="p" data-group-id="0429442747-3">{</span><span class="s">&quot;Hello, y&#39;all!&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;How are you üòÅ ?&quot;</span><span class="p" data-group-id="0429442747-3">}</span><span class="p" data-group-id="0429442747-2">)</span><span class="w">
</span><span class="nc">Encoding</span><span class="o">.</span><span class="n">get_tokens</span><span class="p" data-group-id="0429442747-4">(</span><span class="n">encoding</span><span class="p" data-group-id="0429442747-4">)</span></code></pre><p>You can then check the type IDs attributed to each token is correct with</p><pre><code class="makeup elixir" translate="no"><span class="nc">Encoding</span><span class="o">.</span><span class="n">get_type_ids</span><span class="p" data-group-id="3612861509-1">(</span><span class="n">encoding</span><span class="p" data-group-id="3612861509-1">)</span></code></pre><p>If you save your tokenizer with Tokenizer.save, the post-processor will be saved along.</p><!-- livebook:{"break_markdown":true} --><h3 id="encoding-multiple-sentences-in-a-batch" class="section-heading">
  <a href="#encoding-multiple-sentences-in-a-batch">
    <i class="ri-link-m" aria-hidden="true"></i>
    Encoding multiple sentences in a batch
  </a>
</h3>
<!-- livebook:{"break_markdown":true} --><p>To get the full speed of the ü§ó Tokenizers library, it's best to process your texts by batches by using the Tokenizer.encode_batch method:</p><pre><code class="makeup elixir" translate="no"><span class="p" data-group-id="4530732357-1">{</span><span class="ss">:ok</span><span class="p">,</span><span class="w"> </span><span class="n">encoding</span><span class="p" data-group-id="4530732357-1">}</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nc">Tokenizer</span><span class="o">.</span><span class="n">encode_batch</span><span class="p" data-group-id="4530732357-2">(</span><span class="n">tokenizer</span><span class="p">,</span><span class="w"> </span><span class="p" data-group-id="4530732357-3">[</span><span class="s">&quot;Hello, y&#39;all!&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;How are you üòÅ ?&quot;</span><span class="p" data-group-id="4530732357-3">]</span><span class="p" data-group-id="4530732357-2">)</span></code></pre><p>The output is then a list of Encoding objects like the ones we saw before. You can process together as many texts as you like, as long as it fits in memory.</p><p>To process a batch of sentences pairs, pass two lists to the Tokenizer.encode_batch method: the list of sentences A and the list of sentences B:</p><pre><code class="makeup elixir" translate="no"><span class="p" data-group-id="8967404486-1">{</span><span class="ss">:ok</span><span class="p">,</span><span class="w"> </span><span class="n">encoding</span><span class="p" data-group-id="8967404486-1">}</span><span class="w"> </span><span class="o">=</span><span class="w">
  </span><span class="nc">Tokenizer</span><span class="o">.</span><span class="n">encode_batch</span><span class="p" data-group-id="8967404486-2">(</span><span class="n">tokenizer</span><span class="p">,</span><span class="w"> </span><span class="p" data-group-id="8967404486-3">[</span><span class="w">
    </span><span class="p" data-group-id="8967404486-4">{</span><span class="s">&quot;Hello, y&#39;all!&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;How are you üòÅ ?&quot;</span><span class="p" data-group-id="8967404486-4">}</span><span class="p">,</span><span class="w">
    </span><span class="p" data-group-id="8967404486-5">{</span><span class="w">
      </span><span class="s">&quot;Hello to you too!&quot;</span><span class="p">,</span><span class="w">
      </span><span class="s">&quot;I&#39;m fine, thank you!&quot;</span><span class="w">
    </span><span class="p" data-group-id="8967404486-5">}</span><span class="w">
  </span><span class="p" data-group-id="8967404486-3">]</span><span class="p" data-group-id="8967404486-2">)</span></code></pre><p>When encoding multiple sentences, you can automatically pad the outputs to the longest sentence present by using <code class="inline">Tokenizer.enable_padding</code>, with the <code class="inline">pad_token</code> and its ID (which we can double-check the id for the padding token with <code class="inline">Tokenizer.token_to_id</code> like before):</p><pre><code class="makeup elixir" translate="no"><span class="nc">Tokenizer</span><span class="o">.</span><span class="n">set_padding</span><span class="p" data-group-id="8024066873-1">(</span><span class="n">tokenizer</span><span class="p">,</span><span class="w"> </span><span class="ss">pad_id</span><span class="p">:</span><span class="w"> </span><span class="mi">3</span><span class="p">,</span><span class="w"> </span><span class="ss">pad_token</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;[PAD]&quot;</span><span class="p" data-group-id="8024066873-1">)</span></code></pre><p>We can set the direction of the padding (defaults to the right) or a given length if we want to pad every sample to that specific number (here we leave it unset to pad to the size of the longest text).</p><pre><code class="makeup elixir" translate="no"><span class="p" data-group-id="3433551064-1">{</span><span class="ss">:ok</span><span class="p">,</span><span class="w"> </span><span class="n">encoding</span><span class="p" data-group-id="3433551064-1">}</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nc">Tokenizer</span><span class="o">.</span><span class="n">encode_batch</span><span class="p" data-group-id="3433551064-2">(</span><span class="n">tokenizer</span><span class="p">,</span><span class="w"> </span><span class="p" data-group-id="3433551064-3">[</span><span class="s">&quot;Hello, y&#39;all!&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;How are you üòÅ ?&quot;</span><span class="p" data-group-id="3433551064-3">]</span><span class="p" data-group-id="3433551064-2">)</span><span class="w">

</span><span class="n">encoding</span><span class="w">
</span><span class="o">|&gt;</span><span class="w"> </span><span class="nc">Enum</span><span class="o">.</span><span class="n">at</span><span class="p" data-group-id="3433551064-4">(</span><span class="mi">1</span><span class="p" data-group-id="3433551064-4">)</span><span class="w">
</span><span class="o">|&gt;</span><span class="w"> </span><span class="nc">Encoding</span><span class="o">.</span><span class="n">get_tokens</span><span class="p" data-group-id="3433551064-5">(</span><span class="p" data-group-id="3433551064-5">)</span></code></pre><p>In this case, the <code class="inline">attention mask</code> generated by the tokenizer takes the padding into account:</p><pre><code class="makeup elixir" translate="no"><span class="n">encoding</span><span class="w">
</span><span class="o">|&gt;</span><span class="w"> </span><span class="nc">Enum</span><span class="o">.</span><span class="n">at</span><span class="p" data-group-id="7212747840-1">(</span><span class="mi">1</span><span class="p" data-group-id="7212747840-1">)</span><span class="w">
</span><span class="o">|&gt;</span><span class="w"> </span><span class="nc">Encoding</span><span class="o">.</span><span class="n">get_attention_mask</span><span class="p" data-group-id="7212747840-2">(</span><span class="p" data-group-id="7212747840-2">)</span></code></pre>
<div class="bottom-actions">
  <div class="bottom-actions-item">

      <a href="license.html" class="bottom-actions-button" rel="prev">
        <span class="subheader">
          ‚Üê Previous Page
        </span>
        <span class="title">
LICENSE
        </span>
      </a>

  </div>
  <div class="bottom-actions-item">

      <a href="pretrained.html" class="bottom-actions-button" rel="next">
        <span class="subheader">
          Next Page ‚Üí
        </span>
        <span class="title">
Pretrained Tokenizers
        </span>
      </a>

  </div>
</div>
      <footer class="footer">
        <p>

            <span class="line">
              <a href="https://hex.pm/packages/tokenizers/0.4.0-dev" class="footer-hex-package">Hex Package</a>

              <a href="https://preview.hex.pm/preview/tokenizers/0.4.0-dev">Hex Preview</a>

                (<a href="https://preview.hex.pm/preview/tokenizers/0.4.0-dev/show/notebooks/quicktour.livemd">current file</a>)

            </span>

          <span class="line">
            <button class="a-main footer-button display-quick-switch" title="Search HexDocs packages">
              Search HexDocs
            </button>

              <a href="Tokenizers.epub" title="ePub version">
                Download ePub version
              </a>

          </span>
        </p>

        <p class="built-using">
          Built using
          <a href="https://github.com/elixir-lang/ex_doc" title="ExDoc" target="_blank" rel="help noopener" translate="no">ExDoc</a> (v0.30.2) for the

            <a href="https://elixir-lang.org" title="Elixir" target="_blank" translate="no">Elixir programming language</a>

        </p>
      </footer>
    </div>
  </div>
</section>
</div>


  </body>
</html>
