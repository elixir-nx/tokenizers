<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="generator" content="ExDoc v0.30.4">
    <meta name="project" content="Tokenizers v0.5.0">

    <title>Training custom tokenizer ‚Äî Tokenizers v0.5.0</title>
    <link rel="stylesheet" href="dist/html-elixir-HFL3S4U6.css" />


    <script src="dist/handlebars.runtime-NWIB6V2M.js"></script>
    <script src="dist/handlebars.templates-NBND3S2D.js"></script>
    <script src="dist/sidebar_items-70B02A1A.js"></script>

      <script src="docs_config.js"></script>

    <script async src="dist/html-B3DGGEY7.js"></script>


  </head>
  <body data-type="extras" class="page-livemd">
    <script>

      try {
        var settings = JSON.parse(localStorage.getItem('ex_doc:settings') || '{}');

        if (settings.theme === 'dark' ||
           ((settings.theme === 'system' || settings.theme == null) &&
             window.matchMedia('(prefers-color-scheme: dark)').matches)
           ) {
          document.body.classList.add('dark')
        }
      } catch (error) { }
    </script>

<div class="main">

<button class="sidebar-button sidebar-toggle" aria-label="toggle sidebar">
  <i class="ri-menu-line ri-lg" title="Collapse/expand sidebar"></i>
</button>

<section class="sidebar">
  <form class="sidebar-search" action="search.html">
    <button type="submit" class="search-button" aria-label="Submit Search">
      <i class="ri-search-2-line" aria-hidden="true" title="Submit search"></i>
    </button>
    <button type="button" tabindex="-1" class="search-close-button" aria-label="Cancel Search">
      <i class="ri-close-line ri-lg" aria-hidden="true" title="Cancel search"></i>
    </button>
    <label class="search-label">
      <p class="sr-only">Search</p>
      <input name="q" type="text" class="search-input" placeholder="Search..." aria-label="Input your search terms" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" />
    </label>
  </form>

  <div class="autocomplete">
    <div class="autocomplete-results">
    </div>
  </div>

  <div class="sidebar-header">

    <div class="sidebar-projectDetails">
      <a href="Tokenizers.html" class="sidebar-projectName" translate="no">
Tokenizers
      </a>
      <div class="sidebar-projectVersion" translate="no">
        v0.5.0
      </div>
    </div>
    <ul class="sidebar-listNav">
      <li><a id="extras-list-link" href="#full-list">Pages</a></li>

        <li><a id="modules-list-link" href="#full-list">Modules</a></li>


    </ul>
  </div>

  <div class="gradient"></div>
  <ul id="full-list"></ul>
</section>

<section class="content">
  <output role="status" id="toast"></output>
  <div class="content-outer">
    <div id="content" class="content-inner">

<h1>
<button class="icon-action display-settings">
  <i class="ri-settings-3-line"></i>
  <span class="sr-only">Settings</span>
</button>


    <a href="https://github.com/elixir-nx/tokenizers/blob/v0.5.0/notebooks/training.livemd#L1" title="View Source" class="icon-action" rel="help">
      <i class="ri-code-s-slash-line" aria-hidden="true"></i>
      <span class="sr-only">View Source</span>
    </a>


  <span>Training custom tokenizer</span>
</h1>

  <div class="livebook-badge-container">
    <a href="#" class="livebook-badge">
      <img src="https://livebook.dev/badge/v1/blue.svg" alt="Run in Livebook" width="150" />
    </a>
  </div>

<pre><code class="makeup elixir" translate="no"><span class="nc">Mix</span><span class="o">.</span><span class="n">install</span><span class="p" data-group-id="2755729852-1">(</span><span class="p" data-group-id="2755729852-2">[</span><span class="w">
  </span><span class="p" data-group-id="2755729852-3">{</span><span class="ss">:tokenizers</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;~&gt; 0.4.0&quot;</span><span class="p" data-group-id="2755729852-3">}</span><span class="p">,</span><span class="w">
  </span><span class="p" data-group-id="2755729852-4">{</span><span class="ss">:req</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;~&gt; 0.3.8&quot;</span><span class="p" data-group-id="2755729852-4">}</span><span class="w">
</span><span class="p" data-group-id="2755729852-2">]</span><span class="p" data-group-id="2755729852-1">)</span></code></pre><h2 id="intro" class="section-heading">
  <a href="#intro">
    <i class="ri-link-m" aria-hidden="true"></i>
    Intro
  </a>
</h2>
<p>Let‚Äôs have a quick look at the ü§ó Tokenizers library features. The library provides an implementation of today‚Äôs most used tokenizers that is both easy to use and blazing fast.</p><!-- livebook:{"branch_parent_index":0} --><h2 id="downloading-the-data" class="section-heading">
  <a href="#downloading-the-data">
    <i class="ri-link-m" aria-hidden="true"></i>
    Downloading the data
  </a>
</h2>
<p>To illustrate how fast the ü§ó Tokenizers library is, let‚Äôs train a new tokenizer on wikitext-103 (516M of text) in just a few seconds. First things first, you will need to download this dataset and unzip it with:</p><pre><code class="makeup bash" translate="no"><span class="">wget https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-raw-v1.zip
</span><span class="">unzip wikitext-103-raw-v1.zip
</span></code></pre><!-- livebook:{"break_markdown":true} --><p>Alternatively you can run this code:</p><pre><code class="makeup elixir" translate="no"><span class="nc">Req</span><span class="o">.</span><span class="n">get!</span><span class="p" data-group-id="2310024040-1">(</span><span class="s">&quot;https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-raw-v1.zip&quot;</span><span class="p" data-group-id="2310024040-1">)</span><span class="o">.</span><span class="n">body</span><span class="w">
</span><span class="o">|&gt;</span><span class="w"> </span><span class="nc">Enum</span><span class="o">.</span><span class="n">each</span><span class="p" data-group-id="2310024040-2">(</span><span class="k" data-group-id="2310024040-3">fn</span><span class="w"> </span><span class="p" data-group-id="2310024040-4">{</span><span class="n">filename</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="p" data-group-id="2310024040-4">}</span><span class="w"> </span><span class="o">-&gt;</span><span class="w">
  </span><span class="n">filename</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">to_string</span><span class="p" data-group-id="2310024040-5">(</span><span class="n">filename</span><span class="p" data-group-id="2310024040-5">)</span><span class="w">
  </span><span class="n">path</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nc">Path</span><span class="o">.</span><span class="n">join</span><span class="p" data-group-id="2310024040-6">(</span><span class="bp">__DIR__</span><span class="p">,</span><span class="w"> </span><span class="n">filename</span><span class="p" data-group-id="2310024040-6">)</span><span class="w">
  </span><span class="nc">IO</span><span class="o">.</span><span class="n">puts</span><span class="p" data-group-id="2310024040-7">(</span><span class="s">&quot;Writing </span><span class="si" data-group-id="2310024040-8">#{</span><span class="n">filename</span><span class="si" data-group-id="2310024040-8">}</span><span class="s"> to path </span><span class="si" data-group-id="2310024040-9">#{</span><span class="n">path</span><span class="si" data-group-id="2310024040-9">}</span><span class="s">&quot;</span><span class="p" data-group-id="2310024040-7">)</span><span class="w">

  </span><span class="ss">:ok</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nc">File</span><span class="o">.</span><span class="n">mkdir_p!</span><span class="p" data-group-id="2310024040-10">(</span><span class="nc">Path</span><span class="o">.</span><span class="n">dirname</span><span class="p" data-group-id="2310024040-11">(</span><span class="n">path</span><span class="p" data-group-id="2310024040-11">)</span><span class="p" data-group-id="2310024040-10">)</span><span class="w">
  </span><span class="nc">File</span><span class="o">.</span><span class="n">write!</span><span class="p" data-group-id="2310024040-12">(</span><span class="n">path</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="p">,</span><span class="w"> </span><span class="p" data-group-id="2310024040-13">[</span><span class="ss">:write</span><span class="p" data-group-id="2310024040-13">]</span><span class="p" data-group-id="2310024040-12">)</span><span class="w">
</span><span class="k" data-group-id="2310024040-3">end</span><span class="p" data-group-id="2310024040-2">)</span></code></pre><h2 id="training-the-tokenizer-from-scratch" class="section-heading">
  <a href="#training-the-tokenizer-from-scratch">
    <i class="ri-link-m" aria-hidden="true"></i>
    Training the tokenizer from scratch
  </a>
</h2>
<pre><code class="makeup elixir" translate="no"><span class="kn">alias</span><span class="w"> </span><span class="nc">Tokenizers.Tokenizer</span><span class="w">
</span><span class="kn">alias</span><span class="w"> </span><span class="nc">Tokenizers.Trainer</span><span class="w">
</span><span class="kn">alias</span><span class="w"> </span><span class="nc">Tokenizers.PostProcessor</span><span class="w">
</span><span class="kn">alias</span><span class="w"> </span><span class="nc">Tokenizers.PreTokenizer</span><span class="w">
</span><span class="kn">alias</span><span class="w"> </span><span class="nc">Tokenizers.Model</span><span class="w">
</span><span class="kn">alias</span><span class="w"> </span><span class="nc">Tokenizers.Encoding</span></code></pre><p>In this tour, we will build and train a Byte-Pair Encoding (BPE) tokenizer. For more information about the different type of tokenizers, check out this guide in the ü§ó Transformers documentation. Here, training the tokenizer means it will learn merge rules by:</p><ul><li>Start with all the characters present in the training corpus as tokens.</li><li>Identify the most common pair of tokens and merge it into one token.</li><li>Repeat until the vocabulary (e.g., the number of tokens) has reached the size we want.</li></ul><p>The main API of the library is the class Tokenizer, here is how we instantiate one with a BPE model:</p><pre><code class="makeup elixir" translate="no"><span class="p" data-group-id="0860009174-1">{</span><span class="ss">:ok</span><span class="p">,</span><span class="w"> </span><span class="n">model</span><span class="p" data-group-id="0860009174-1">}</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nc">Model.BPE</span><span class="o">.</span><span class="n">init</span><span class="p" data-group-id="0860009174-2">(</span><span class="p" data-group-id="0860009174-3">%{</span><span class="p" data-group-id="0860009174-3">}</span><span class="p">,</span><span class="w"> </span><span class="p" data-group-id="0860009174-4">[</span><span class="p" data-group-id="0860009174-4">]</span><span class="p">,</span><span class="w"> </span><span class="ss">unk_token</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;[UNK]&quot;</span><span class="p" data-group-id="0860009174-2">)</span><span class="w">
</span><span class="p" data-group-id="0860009174-5">{</span><span class="ss">:ok</span><span class="p">,</span><span class="w"> </span><span class="n">tokenizer</span><span class="p" data-group-id="0860009174-5">}</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nc">Tokenizer</span><span class="o">.</span><span class="n">init</span><span class="p" data-group-id="0860009174-6">(</span><span class="n">model</span><span class="p" data-group-id="0860009174-6">)</span></code></pre><p>To train our tokenizer on the wikitext files, we will need to instantiate a <strong>trainer</strong>, in this case a BPE trainer:</p><pre><code class="makeup elixir" translate="no"><span class="p" data-group-id="9952081488-1">{</span><span class="ss">:ok</span><span class="p">,</span><span class="w"> </span><span class="n">trainer</span><span class="p" data-group-id="9952081488-1">}</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nc">Trainer</span><span class="o">.</span><span class="n">bpe</span><span class="p" data-group-id="9952081488-2">(</span><span class="ss">special_tokens</span><span class="p">:</span><span class="w"> </span><span class="p" data-group-id="9952081488-3">[</span><span class="s">&quot;[UNK]&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;[CLS]&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;[SEP]&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;[PAD]&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;[MASK]&quot;</span><span class="p" data-group-id="9952081488-3">]</span><span class="p" data-group-id="9952081488-2">)</span></code></pre><p>We can set the training arguments like <code class="inline">vocab_size</code> or <code class="inline">min_frequency</code> (here left at their default values of <code class="inline">30,000</code> and <code class="inline">0</code>), but the most important part is to give the <code class="inline">special_tokens</code> we plan to use later on (they are not used at all during training) so that they get inserted in the vocabulary.</p><blockquote><p>The order in which you write the special tokens list matters: here <code class="inline">&quot;[UNK]&quot;</code> will get the ID <code class="inline">0</code>, <code class="inline">&quot;[CLS]&quot;</code> will get the ID <code class="inline">1</code> and so forth.</p></blockquote><p>We could train our tokenizer right now, but it wouldn't be optimal. Without a pre-tokenizer that will split our inputs into words, we might get tokens that overlap several words: for instance we could get an &quot;it is&quot; token since those two words often appear next to each other. Using a pre-tokenizer will ensure no token is bigger than a word returned by the pre-tokenizer. Here we want to train a subword BPE tokenizer, and we will use the easiest pre-tokenizer possible by splitting on whitespace.</p><pre><code class="makeup elixir" translate="no"><span class="n">tokenizer</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nc">Tokenizer</span><span class="o">.</span><span class="n">set_pre_tokenizer</span><span class="p" data-group-id="0548537572-1">(</span><span class="n">tokenizer</span><span class="p">,</span><span class="w"> </span><span class="nc">PreTokenizer</span><span class="o">.</span><span class="n">whitespace</span><span class="p" data-group-id="0548537572-2">(</span><span class="p" data-group-id="0548537572-2">)</span><span class="p" data-group-id="0548537572-1">)</span></code></pre><p>Now, we can just call the <code class="inline">Tokenizer.train_from_files/3</code> function with the list of files we want to train on:</p><pre><code class="makeup elixir" translate="no"><span class="p" data-group-id="8145865113-1">{</span><span class="ss">:ok</span><span class="p">,</span><span class="w"> </span><span class="n">tokenizer</span><span class="p" data-group-id="8145865113-1">}</span><span class="w"> </span><span class="o">=</span><span class="w">
  </span><span class="p" data-group-id="8145865113-2">[</span><span class="w">
    </span><span class="s">&quot;wikitext-103-raw/wiki.test.raw&quot;</span><span class="p">,</span><span class="w">
    </span><span class="s">&quot;wikitext-103-raw/wiki.train.raw&quot;</span><span class="p">,</span><span class="w">
    </span><span class="s">&quot;wikitext-103-raw/wiki.valid.raw&quot;</span><span class="w">
  </span><span class="p" data-group-id="8145865113-2">]</span><span class="w">
  </span><span class="o">|&gt;</span><span class="w"> </span><span class="nc">Enum</span><span class="o">.</span><span class="n">map</span><span class="p" data-group-id="8145865113-3">(</span><span class="o">&amp;</span><span class="nc">Path</span><span class="o">.</span><span class="n">join</span><span class="p" data-group-id="8145865113-4">(</span><span class="bp">__DIR__</span><span class="p">,</span><span class="w"> </span><span class="ni">&amp;1</span><span class="p" data-group-id="8145865113-4">)</span><span class="p" data-group-id="8145865113-3">)</span><span class="w">
  </span><span class="o">|&gt;</span><span class="w"> </span><span class="n">then</span><span class="p" data-group-id="8145865113-5">(</span><span class="o">&amp;</span><span class="nc">Tokenizer</span><span class="o">.</span><span class="n">train_from_files</span><span class="p" data-group-id="8145865113-6">(</span><span class="n">tokenizer</span><span class="p">,</span><span class="w"> </span><span class="ni">&amp;1</span><span class="p">,</span><span class="w"> </span><span class="ss">trainer</span><span class="p">:</span><span class="w"> </span><span class="n">trainer</span><span class="p" data-group-id="8145865113-6">)</span><span class="p" data-group-id="8145865113-5">)</span></code></pre><p>This should only take a few seconds to train our tokenizer on the full wikitext dataset! To save the tokenizer in one file that contains all its configuration and vocabulary, just use the <code class="inline">Tokenizer.save/2</code> function:</p><pre><code class="makeup elixir" translate="no"><span class="nc">Tokenizer</span><span class="o">.</span><span class="n">save</span><span class="p" data-group-id="2252544251-1">(</span><span class="n">tokenizer</span><span class="p">,</span><span class="w"> </span><span class="nc">Path</span><span class="o">.</span><span class="n">join</span><span class="p" data-group-id="2252544251-2">(</span><span class="bp">__DIR__</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;tokenizer-wiki.json&quot;</span><span class="p" data-group-id="2252544251-2">)</span><span class="p" data-group-id="2252544251-1">)</span></code></pre><p>and you can reload your tokenizer from that file with the <code class="inline">Tokenizer.from_file/1</code> function:</p><pre><code class="makeup elixir" translate="no"><span class="p" data-group-id="6236245601-1">{</span><span class="ss">:ok</span><span class="p">,</span><span class="w"> </span><span class="n">tokenizer</span><span class="p" data-group-id="6236245601-1">}</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nc">Tokenizer</span><span class="o">.</span><span class="n">from_file</span><span class="p" data-group-id="6236245601-2">(</span><span class="nc">Path</span><span class="o">.</span><span class="n">join</span><span class="p" data-group-id="6236245601-3">(</span><span class="bp">__DIR__</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;tokenizer-wiki.json&quot;</span><span class="p" data-group-id="6236245601-3">)</span><span class="p" data-group-id="6236245601-2">)</span></code></pre><h2 id="using-the-tokenizer" class="section-heading">
  <a href="#using-the-tokenizer">
    <i class="ri-link-m" aria-hidden="true"></i>
    Using the tokenizer
  </a>
</h2>
<p>Now that we have trained a tokenizer, we can use it on any text we want with the <code class="inline">Tokenizer.encode/1</code> function:</p><pre><code class="makeup elixir" translate="no"><span class="p" data-group-id="3793208706-1">{</span><span class="ss">:ok</span><span class="p">,</span><span class="w"> </span><span class="n">encoding</span><span class="p" data-group-id="3793208706-1">}</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nc">Tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p" data-group-id="3793208706-2">(</span><span class="n">tokenizer</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;Hello, y&#39;all! How are you üòÅ ?&quot;</span><span class="p" data-group-id="3793208706-2">)</span></code></pre><p>This applied the full pipeline of the tokenizer on the text, returning an <code class="inline">encoding</code>. To learn more about this pipeline, and how to apply (or customize) parts of it, check out <a href="https://huggingface.co/docs/tokenizers/pipeline">this page</a>.</p><p>This <code class="inline">encoding</code> then has all the attributes you need for your deep learning model (or other). The tokens attribute contains the segmentation of your text in tokens:</p><pre><code class="makeup elixir" translate="no"><span class="nc">Encoding</span><span class="o">.</span><span class="n">get_tokens</span><span class="p" data-group-id="1801412150-1">(</span><span class="n">encoding</span><span class="p" data-group-id="1801412150-1">)</span></code></pre><p>Similarly, the ids attribute will contain the index of each of those tokens in the tokenizer‚Äôs vocabulary:</p><pre><code class="makeup elixir" translate="no"><span class="nc">Encoding</span><span class="o">.</span><span class="n">get_ids</span><span class="p" data-group-id="7030898448-1">(</span><span class="n">encoding</span><span class="p" data-group-id="7030898448-1">)</span></code></pre><p>An important feature of the ü§ó Tokenizers library is that it comes with full alignment tracking, meaning you can always get the part of your original sentence that corresponds to a given token. Those are stored in the offsets attribute of our Encoding object. For instance, let‚Äôs assume we would want to find back what caused the &quot;[UNK]&quot; token to appear, which is the token at index 9 in the list, we can just ask for the offset at the index:</p><pre><code class="makeup elixir" translate="no"><span class="p" data-group-id="7540144213-1">{</span><span class="n">emoji_offset_start</span><span class="p">,</span><span class="w"> </span><span class="n">emoji_offset_end</span><span class="p" data-group-id="7540144213-1">}</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nc">Encoding</span><span class="o">.</span><span class="n">get_offsets</span><span class="p" data-group-id="7540144213-2">(</span><span class="n">encoding</span><span class="p" data-group-id="7540144213-2">)</span><span class="w"> </span><span class="o">|&gt;</span><span class="w"> </span><span class="nc">Enum</span><span class="o">.</span><span class="n">at</span><span class="p" data-group-id="7540144213-3">(</span><span class="mi">9</span><span class="p" data-group-id="7540144213-3">)</span></code></pre><p>and those are the indices that correspond to the emoji in the original sentence:</p><pre><code class="makeup elixir" translate="no"><span class="nc">:binary</span><span class="o">.</span><span class="n">part</span><span class="p" data-group-id="3120541798-1">(</span><span class="w">
  </span><span class="s">&quot;Hello, y&#39;all! How are you üòÅ ?&quot;</span><span class="p">,</span><span class="w">
  </span><span class="n">emoji_offset_start</span><span class="p">,</span><span class="w">
  </span><span class="c1"># Length</span><span class="w">
  </span><span class="n">emoji_offset_end</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">emoji_offset_start</span><span class="w">
</span><span class="p" data-group-id="3120541798-1">)</span></code></pre><h2 id="post-processing" class="section-heading">
  <a href="#post-processing">
    <i class="ri-link-m" aria-hidden="true"></i>
    Post-processing
  </a>
</h2>
<p>We might want our tokenizer to automatically add special tokens, like <code class="inline">[CLS]</code> or <code class="inline">[SEP]</code>. To do this, we use a post-processor. Template post-processing is the most commonly used, you just have to specify a template for the processing of single sentences and pairs of sentences, along with the special tokens and their IDs.</p><p>When we built our tokenizer, we set <code class="inline">[CLS]</code> and <code class="inline">[SEP]</code> in positions 1 and 2 of our list of special tokens, so this should be their IDs. To double-check, we can use the <code class="inline">Tokenizer.token_to_id/2</code> function:</p><pre><code class="makeup elixir" translate="no"><span class="nc">Tokenizer</span><span class="o">.</span><span class="n">token_to_id</span><span class="p" data-group-id="0123826523-1">(</span><span class="n">tokenizer</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;[SEP]&quot;</span><span class="p" data-group-id="0123826523-1">)</span></code></pre><p>Here is how we can set the post-processing to give us the traditional BERT inputs:</p><pre><code class="makeup elixir" translate="no"><span class="n">tokenizer</span><span class="w"> </span><span class="o">=</span><span class="w">
  </span><span class="nc">Tokenizer</span><span class="o">.</span><span class="n">set_post_processor</span><span class="p" data-group-id="4594188728-1">(</span><span class="w">
    </span><span class="n">tokenizer</span><span class="p">,</span><span class="w">
    </span><span class="nc">PostProcessor</span><span class="o">.</span><span class="n">template</span><span class="p" data-group-id="4594188728-2">(</span><span class="w">
      </span><span class="ss">single</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;[CLS] $A [SEP]&quot;</span><span class="p">,</span><span class="w">
      </span><span class="ss">pair</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;[CLS] $A [SEP] $B:1 [SEP]:1&quot;</span><span class="p">,</span><span class="w">
      </span><span class="ss">special_tokens</span><span class="p">:</span><span class="w"> </span><span class="p" data-group-id="4594188728-3">[</span><span class="w">
        </span><span class="p" data-group-id="4594188728-4">{</span><span class="s">&quot;[CLS]&quot;</span><span class="p">,</span><span class="w"> </span><span class="nc">Tokenizer</span><span class="o">.</span><span class="n">token_to_id</span><span class="p" data-group-id="4594188728-5">(</span><span class="n">tokenizer</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;[CLS]&quot;</span><span class="p" data-group-id="4594188728-5">)</span><span class="p" data-group-id="4594188728-4">}</span><span class="p">,</span><span class="w">
        </span><span class="p" data-group-id="4594188728-6">{</span><span class="s">&quot;[SEP]&quot;</span><span class="p">,</span><span class="w"> </span><span class="nc">Tokenizer</span><span class="o">.</span><span class="n">token_to_id</span><span class="p" data-group-id="4594188728-7">(</span><span class="n">tokenizer</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;[SEP]&quot;</span><span class="p" data-group-id="4594188728-7">)</span><span class="p" data-group-id="4594188728-6">}</span><span class="w">
      </span><span class="p" data-group-id="4594188728-3">]</span><span class="w">
    </span><span class="p" data-group-id="4594188728-2">)</span><span class="w">
  </span><span class="p" data-group-id="4594188728-1">)</span></code></pre><p>Let's go over this snippet of code in more details. First we specify the template for single sentences: those should have the form <code class="inline">&quot;[CLS] $A [SEP]&quot;</code> where <code class="inline">$A</code> represents our sentence.</p><p>Then, we specify the template for sentence pairs, which should have the form <code class="inline">&quot;[CLS] $A [SEP] $B [SEP]&quot;</code> where <code class="inline">$A</code> represents the first sentence and <code class="inline">$B</code> the second one. The <code class="inline">:1</code> added in the template represent the type IDs we want for each part of our input: it defaults to <code class="inline">0</code> for everything (which is why we don't have <code class="inline">$A:0</code>) and here we set it to 1 for the tokens of the second sentence and the last <code class="inline">&quot;[SEP]&quot;</code> token.</p><p>Lastly, we specify the special tokens we used and their IDs in our tokenizer's vocabulary.</p><p>To check out this worked properly, let's try to encode the same sentence as before:</p><pre><code class="makeup elixir" translate="no"><span class="p" data-group-id="5512031827-1">{</span><span class="ss">:ok</span><span class="p">,</span><span class="w"> </span><span class="n">encoding</span><span class="p" data-group-id="5512031827-1">}</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nc">Tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p" data-group-id="5512031827-2">(</span><span class="n">tokenizer</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;Hello, y&#39;all! How are you üòÅ ?&quot;</span><span class="p" data-group-id="5512031827-2">)</span><span class="w">
</span><span class="nc">Encoding</span><span class="o">.</span><span class="n">get_tokens</span><span class="p" data-group-id="5512031827-3">(</span><span class="n">encoding</span><span class="p" data-group-id="5512031827-3">)</span></code></pre><p>To check the results on a pair of sentences, we just pass the two sentences to <code class="inline">Tokenizer.encode/2</code>:</p><pre><code class="makeup elixir" translate="no"><span class="p" data-group-id="8185646768-1">{</span><span class="ss">:ok</span><span class="p">,</span><span class="w"> </span><span class="n">encoding</span><span class="p" data-group-id="8185646768-1">}</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nc">Tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p" data-group-id="8185646768-2">(</span><span class="n">tokenizer</span><span class="p">,</span><span class="w"> </span><span class="p" data-group-id="8185646768-3">{</span><span class="s">&quot;Hello, y&#39;all!&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;How are you üòÅ ?&quot;</span><span class="p" data-group-id="8185646768-3">}</span><span class="p" data-group-id="8185646768-2">)</span><span class="w">
</span><span class="nc">Encoding</span><span class="o">.</span><span class="n">get_tokens</span><span class="p" data-group-id="8185646768-4">(</span><span class="n">encoding</span><span class="p" data-group-id="8185646768-4">)</span></code></pre><p>You can then check the type IDs attributed to each token is correct with</p><pre><code class="makeup elixir" translate="no"><span class="nc">Encoding</span><span class="o">.</span><span class="n">get_type_ids</span><span class="p" data-group-id="5311810228-1">(</span><span class="n">encoding</span><span class="p" data-group-id="5311810228-1">)</span></code></pre><p>If you save your tokenizer with <code class="inline">Tokenizer.save/2</code>, the post-processor will be saved along.</p><h2 id="encoding-multiple-sentences-in-a-batch" class="section-heading">
  <a href="#encoding-multiple-sentences-in-a-batch">
    <i class="ri-link-m" aria-hidden="true"></i>
    Encoding multiple sentences in a batch
  </a>
</h2>
<p>To get the full speed of the ü§ó Tokenizers library, it's best to process your texts by batches by using the <code class="inline">Tokenizer.encode_batch/2</code> function:</p><pre><code class="makeup elixir" translate="no"><span class="p" data-group-id="4970399759-1">{</span><span class="ss">:ok</span><span class="p">,</span><span class="w"> </span><span class="n">encoding</span><span class="p" data-group-id="4970399759-1">}</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nc">Tokenizer</span><span class="o">.</span><span class="n">encode_batch</span><span class="p" data-group-id="4970399759-2">(</span><span class="n">tokenizer</span><span class="p">,</span><span class="w"> </span><span class="p" data-group-id="4970399759-3">[</span><span class="s">&quot;Hello, y&#39;all!&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;How are you üòÅ ?&quot;</span><span class="p" data-group-id="4970399759-3">]</span><span class="p" data-group-id="4970399759-2">)</span></code></pre><p>The output is then a list of <code class="inline">encoding</code>s like the ones we saw before. You can process together as many texts as you like, as long as it fits in memory.</p><p>To process a batch of sentence pairs, pass a list of tuples to the <code class="inline">Tokenizer.encode_batch/2</code> function:</p><pre><code class="makeup elixir" translate="no"><span class="p" data-group-id="2125585882-1">{</span><span class="ss">:ok</span><span class="p">,</span><span class="w"> </span><span class="n">encoding</span><span class="p" data-group-id="2125585882-1">}</span><span class="w"> </span><span class="o">=</span><span class="w">
  </span><span class="nc">Tokenizer</span><span class="o">.</span><span class="n">encode_batch</span><span class="p" data-group-id="2125585882-2">(</span><span class="n">tokenizer</span><span class="p">,</span><span class="w"> </span><span class="p" data-group-id="2125585882-3">[</span><span class="w">
    </span><span class="p" data-group-id="2125585882-4">{</span><span class="s">&quot;Hello, y&#39;all!&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;How are you üòÅ ?&quot;</span><span class="p" data-group-id="2125585882-4">}</span><span class="p">,</span><span class="w">
    </span><span class="p" data-group-id="2125585882-5">{</span><span class="w">
      </span><span class="s">&quot;Hello to you too!&quot;</span><span class="p">,</span><span class="w">
      </span><span class="s">&quot;I&#39;m fine, thank you!&quot;</span><span class="w">
    </span><span class="p" data-group-id="2125585882-5">}</span><span class="w">
  </span><span class="p" data-group-id="2125585882-3">]</span><span class="p" data-group-id="2125585882-2">)</span></code></pre><p>When encoding multiple sentences, you can automatically pad the outputs to the longest sentence present by using <code class="inline">Tokenizer.set_padding/2</code>, with the <code class="inline">pad_token</code> and its ID (which we can double-check the id for the padding token with <code class="inline">Tokenizer.token_to_id/2</code> like before):</p><pre><code class="makeup elixir" translate="no"><span class="n">tokenizer</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nc">Tokenizer</span><span class="o">.</span><span class="n">set_padding</span><span class="p" data-group-id="0885450097-1">(</span><span class="n">tokenizer</span><span class="p">,</span><span class="w"> </span><span class="ss">pad_id</span><span class="p">:</span><span class="w"> </span><span class="mi">3</span><span class="p">,</span><span class="w"> </span><span class="ss">pad_token</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;[PAD]&quot;</span><span class="p" data-group-id="0885450097-1">)</span></code></pre><p>We can set the direction of the padding (defaults to the right) or a given length if we want to pad every sample to that specific number (here we leave it unset to pad to the size of the longest text).</p><pre><code class="makeup elixir" translate="no"><span class="p" data-group-id="1746479996-1">{</span><span class="ss">:ok</span><span class="p">,</span><span class="w"> </span><span class="n">encoding</span><span class="p" data-group-id="1746479996-1">}</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nc">Tokenizer</span><span class="o">.</span><span class="n">encode_batch</span><span class="p" data-group-id="1746479996-2">(</span><span class="n">tokenizer</span><span class="p">,</span><span class="w"> </span><span class="p" data-group-id="1746479996-3">[</span><span class="s">&quot;Hello, y&#39;all!&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;How are you üòÅ ?&quot;</span><span class="p" data-group-id="1746479996-3">]</span><span class="p" data-group-id="1746479996-2">)</span><span class="w">

</span><span class="n">encoding</span><span class="w">
</span><span class="o">|&gt;</span><span class="w"> </span><span class="nc">Enum</span><span class="o">.</span><span class="n">at</span><span class="p" data-group-id="1746479996-4">(</span><span class="mi">1</span><span class="p" data-group-id="1746479996-4">)</span><span class="w">
</span><span class="o">|&gt;</span><span class="w"> </span><span class="nc">Encoding</span><span class="o">.</span><span class="n">get_tokens</span><span class="p" data-group-id="1746479996-5">(</span><span class="p" data-group-id="1746479996-5">)</span></code></pre><p>In this case, the attention mask generated by the tokenizer takes the padding into account:</p><pre><code class="makeup elixir" translate="no"><span class="n">encoding</span><span class="w">
</span><span class="o">|&gt;</span><span class="w"> </span><span class="nc">Enum</span><span class="o">.</span><span class="n">at</span><span class="p" data-group-id="9281777672-1">(</span><span class="mi">1</span><span class="p" data-group-id="9281777672-1">)</span><span class="w">
</span><span class="o">|&gt;</span><span class="w"> </span><span class="nc">Encoding</span><span class="o">.</span><span class="n">get_attention_mask</span><span class="p" data-group-id="9281777672-2">(</span><span class="p" data-group-id="9281777672-2">)</span></code></pre>
<div class="bottom-actions">
  <div class="bottom-actions-item">

      <a href="pretrained.html" class="bottom-actions-button" rel="prev">
        <span class="subheader">
          ‚Üê Previous Page
        </span>
        <span class="title">
Pretrained tokenizers
        </span>
      </a>

  </div>
  <div class="bottom-actions-item">

      <a href="license.html" class="bottom-actions-button" rel="next">
        <span class="subheader">
          Next Page ‚Üí
        </span>
        <span class="title">
LICENSE
        </span>
      </a>

  </div>
</div>
      <footer class="footer">
        <p>

            <span class="line">
              <a href="https://hex.pm/packages/tokenizers/0.5.0" class="footer-hex-package">Hex Package</a>

              <a href="https://preview.hex.pm/preview/tokenizers/0.5.0">Hex Preview</a>

                (<a href="https://preview.hex.pm/preview/tokenizers/0.5.0/show/notebooks/training.livemd">current file</a>)

            </span>

          <span class="line">
            <button class="a-main footer-button display-quick-switch" title="Search HexDocs packages">
              Search HexDocs
            </button>

              <a href="Tokenizers.epub" title="ePub version">
                Download ePub version
              </a>

          </span>
        </p>

        <p class="built-using">
          Built using
          <a href="https://github.com/elixir-lang/ex_doc" title="ExDoc" target="_blank" rel="help noopener" translate="no">ExDoc</a> (v0.30.4) for the

            <a href="https://elixir-lang.org" title="Elixir" target="_blank" translate="no">Elixir programming language</a>

        </p>
      </footer>
    </div>
  </div>
</section>
</div>


  </body>
</html>
